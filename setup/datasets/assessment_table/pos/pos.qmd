---
title: "Period of Support (POS) Sort Index File"
author: "Jerome Dixon"
execute: 
  eval: false
format: 
  html: 
    link-external-icon: true
    link-external-newwindow: true
    toc: true
    toc-depth: 5
    code-fold: true
    code-summary: "Show the code"
    embed-resources: true
    default-image-extension: svg
    dpi: 600
    theme: cosmo
---

```{r libraries}

library(aws.s3)
library(dplyr)
library(readr)
library(DT)
library(purrr)
library(tidyr)
library(jsonlite)
library(here)
library(reticulate)

# For bash scripts
Sys.setenv(assessment_id = "10910")
Sys.setenv(bucket_assessment = "wrsa-dev.canallc.com")
Sys.setenv(wrsa_account_id = "689967901690")
Sys.setenv(imat_account_id = "548995328310")
Sys.setenv(bucket_manifests = "manifests-quicksight")
Sys.setenv(bucket_table_maps = "imat-table-map--use1-az6--x-s3")
Sys.setenv(owner_arn="arn:aws:quicksight:us-east-1:548995328310:user/default/AWSReservedSSO_AdministratorAccess_38a624caf67c613f/jdixon3874")
Sys.setenv(imat_assessments_s3 = "wrsa-dev.canallc.com/assessments")
Sys.setenv(aws_region = "us-east-1")

assessment_id <- Sys.getenv("assessment_id")
assessment_bucket <- Sys.getenv("bucket_assessment")

```

```{python}

# Specify the bucket name and assessment ID
bucket_name = r.assessment_bucket
assessment_id = r.assessment_id

```

# Requirement: Sort dataframes in AWS QuickSight based on index file rather than native alphanumeric sort. 'Beyond' value causes native sort to be incorrect.

Solution: Get POS from AssessmentTableParameterMap. Get OperationDuration from AssessmentTable. BEYOND will equal OperationDuration value.

### Assessment Tables

```{python}

import pandas as pd
import re
import boto3
from io import StringIO

s3_client = boto3.client('s3')

def read_s3_csv(s3_client, bucket_name, object_key):
    """Utility function to read CSV from S3."""
    obj = s3_client.get_object(Bucket=bucket_name, Key=object_key)
    return pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')))
  
# Define file paths
input_file_path1 = f'assessments/{assessment_id}/cos-calculators/AssessmentTable.csv'
input_file_path2 = f'assessments/{assessment_id}/cos-calculators/AssessmentParameterDataMapTable.csv'
output_file_path = f'assessments/{assessment_id}/cos-calculators/pos.csv'

# Read data from S3
df1 = read_s3_csv(s3_client, bucket_name, input_file_path1)
df2 = read_s3_csv(s3_client, bucket_name, input_file_path2)
output_file_path = f'assessments/{assessment_id}/cos-calculators/pos.csv'

merged_df = pd.merge(df1, df2, right_on='AssessmentID', left_on='AssessmentNumber')

```

#### POS Lambda Function

```{python}

import pandas as pd
import re
import boto3
from io import StringIO


def read_s3_csv(s3_client, bucket_name, object_key):
    """Utility function to read CSV from S3."""
    obj = s3_client.get_object(Bucket=bucket_name, Key=object_key)
    return pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')))
  

def getPOS(bucket_name, assessment_id):
    s3_client = boto3.client('s3')
    
    # Define file paths
    input_file_path1 = f'assessments/{assessment_id}/cos-calculators/AssessmentTable.csv'
    input_file_path2 = f'assessments/{assessment_id}/cos-calculators/AssessmentParameterDataMapTable.csv'
    output_file_path = f'assessments/{assessment_id}/cos-calculators/pos.csv'

    # Check for the existence of the POS.CSV file
    try:
        s3_client.head_object(Bucket=bucket_name, Key=output_file_path)
        print("POS.csv file already exists")
        return
    except s3_client.exceptions.ClientError as e:
        if e.response['Error']['Code'] == '404':
            print("POS.csv file does not exist, creating...")
        else:
            raise

    # Read data from S3
    df1 = read_s3_csv(s3_client, bucket_name, input_file_path1)
    df2 = read_s3_csv(s3_client, bucket_name, input_file_path2)
    
    # Merge the dataframes on ID
    merged_df = pd.merge(df1, df2, right_on='AssessmentID', left_on='AssessmentNumber')
    
    # Create the new dataframe with the specified columns and format
    new_data = {
        'POS': ['POS1', 'POS2', 'POS3', 'BEYOND'],
        'DOS': ['DOS1', 'DOS2', 'DOS3', 'BEYOND'],
        'Period': [
            merged_df['DaysOfSupplyFirst'].iloc[0],
            merged_df['DaysOfSupplySecond'].iloc[0],
            merged_df['DaysOfSupplyThird'].iloc[0],
            merged_df['OperationDuration'].iloc[0]
        ],
        'Sort_Index': [1, 2, 3, 4]
    }
    
    pos_df = pd.DataFrame(new_data)
    
    # Convert DataFrame to CSV and upload to S3
    csv_buffer = StringIO()
    pos_df.to_csv(csv_buffer, index=False)
    s3_client.put_object(Bucket=bucket_name, Key=output_file_path, Body=csv_buffer.getvalue())
    print("POS.csv file has been uploaded to S3.")

```

```{python}

# Specify the bucket name and assessment ID
bucket_name = r.assessment_bucket
assessment_id = r.assessment_id

# Call the function
getPOS(bucket_name, assessment_id)

```

```{r}

bucket <- "wrsa-dev.canallc.com"
path <- "assessments/10910/cos-calculators/pos.csv"


pos <- s3read_using(FUN = read.csv, object = path, bucket = bucket)

pos

```

