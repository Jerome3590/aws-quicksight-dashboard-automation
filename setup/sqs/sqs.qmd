---
title: "SQS"
author: "Jerome Dixon"
execute: 
  eval: false
format: 
  html: 
    link-external-icon: true
    link-external-newwindow: true
    toc: true
    toc-depth: 5
    code-fold: true
    code-summary: "Show the code"
    embed-resources: true
    default-image-extension: svg
    dpi: 600
    theme: cosmo
---


```{bash}

aws sqs create-queue --queue-name mushin-imat-datasets.fifo --attributes FifoQueue=true,ContentBasedDeduplication=true --profile mushin

```

### Forward to SQS via S3 Event Notification

```{json}
{
  "Version": "2012-10-17",
  "Id": "arn:aws:sqs:us-east-1:xxxxxxxxxxxx:mushin-imat-datasets.fifo/SQSs3Policy",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "s3.amazonaws.com"
      },
      "Action": "sqs:SendMessage",
      "Resource": "arn:aws:sqs:us-east-1:xxxxxxxxxxxx:mushin-imat-datasets.fifo",
      "Condition": {
        "ArnLike": {
          "aws:SourceArn": "arn:aws:s3:::imat-manifests"
        }
      }
    }
  ]
}


```

![SQS Access Policy Update from SQS AWS Console](../../README/sqs_access_policy.png)


```{bash}

# Create the JSON file
cat <<EOL > s3_sqs_notification.json
{
  "QueueConfigurations": [
    {
      "Id": "s3SQSPolicy",
      "QueueArn": "arn:aws:sqs:us-east-1:xxxxxxxxxxxx:mushin-imat-datasets.fifo",
      "Events": ["s3:ObjectCreated:*"]
    }
  ]
}

EOL


```


```{bash} 

aws s3api put-bucket-notification-configuration --bucket imat-manifests --notification-configuration file://s3_sqs_notification.json --profile mushin

echo "Notification configuration applied successfully."

```

### SQS Processor Lambda Function

```{python}

import boto3
import urllib.parse
import os
import logging
import json
import time
from datetime import datetime
from botocore.exceptions import ClientError
from files import *
from data_sources import *
from datasets import *
from dashboard import *

# Set up logging
logger = logging.getLogger()
logger.setLevel(logging.DEBUG)  # Set to DEBUG to capture all log levels

class DateTimeEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.isoformat()
        return super(DateTimeEncoder, self).default(obj)

# Initialize AWS clients outside the Lambda handler for reuse
s3_client = boto3.client('s3')
quicksight_client = boto3.client('quicksight')

def lambda_handler(event, context):
    # Retrieve environment variables
    aws_account_id = os.environ.get('AWS_ACCOUNT_ID', 'xxxxxxxxxxxx')
    manifest_bucket = os.environ.get('MANIFEST_BUCKET', 'manifests-quicksight')
    assessment_bucket = os.environ.get('ASSESSMENT_BUCKET', '{bucket_name}')
    table_map_bucket = os.environ.get('TABLE_MAP_BUCKET', 'imat-table-map--use1-az6--x-s3')
    template_nbr = os.environ.get('ASSESSMENT_NUMBER', 'template')
    owner_arn = os.environ.get('OWNER_ARN', 'arn:aws:quicksight:us-east-1:xxxxxxxxxxxx:user/default/AWSReservedSSO_AdministratorAccess_38a624caf67c613f/jdixon3874')

    logger.info("Pre-Processing Datasets Started")

    for record in event['Records']:
        # Parse the message from SQS
        message = json.loads(record['body'])
        bucket_name = message['Records'][0]['s3']['bucket']['name']
        object_key = message['Records'][0]['s3']['object']['key']

        if not object_key.lower().endswith('.json'):
            logger.warning(f"Non-json file found: {object_key}")
            continue

        logger.info(f"Processing manifest: {object_key}")
        parts = object_key.split('/')
        assessment_id = parts[0]

        manifest_file_name = parts[1]
        manifest_file_name_without_ext = os.path.splitext(manifest_file_name)[0]

        file_id = construct_file_id(object_key, assessment_id)
        file_name = file_id.replace('-', '_')

        datasource_name = f"{file_name}_datasource_{assessment_id}"
        datasource_id = f"{file_id}-datasource-{assessment_id}"

        dashboard_id = f"RD-Assessment-{assessment_id}"
        dashboard_name = f"RD_Assessment_{assessment_id}"

        dataset_name = f"{file_name}_dataset_{assessment_id}"
        dataset_id = f"{file_id}-dataset-{assessment_id}"
        template_dataset_id = f"{file_id}-dataset-{template_nbr}"
        
        cos_embark_key = f'assessments/{assessment_id}/cos-calculators/aggregated_embark.csv'
        logger.info(f"cos_embark_key: {cos_embark_key}")

        logger.info(f"assessment_id: {assessment_id}")
        logger.info(f"manifest_file_name: {manifest_file_name}")
        logger.info(f"manifest_file_name_without_ext: {manifest_file_name_without_ext}")
        logger.info(f"object_key: {object_key}")
        logger.info(f"file_id: {file_id}")
        logger.info(f"file_name: {file_name}")
        logger.info(f"datasource_name: {datasource_name}")
        logger.info(f"datasource_id: {datasource_id}")
        logger.info(f"dashboard_id: {dashboard_id}")
        logger.info(f"dashboard_name: {dashboard_name}")
        logger.info(f"dataset_name: {dataset_name}")
        logger.info(f"dataset_id: {dataset_id}")
        logger.info(f"Using Assessment#: {template_nbr} For Dashboard Template")
        logger.info(f"Template dataset_id: {template_dataset_id}")

        # Check if table maps exist for file being processed
        logger.info(f"Checking table maps for file: {file_name}")
        if not check_table_map(s3_client, table_map_bucket, file_name):
            logger.error(f"Table maps do not exist for file: {file_name}. Exiting.")
            return {
                'statusCode': 404,
                'body': f'Table maps do not exist for file: {file_name}. Exiting.'
            }

        # Get COS datasets
        logging.info(f"Bucket name: {assessment_bucket}")
        logging.info(f"Object key: {cos_embark_key}")
        try:
            cos_datasets = read_class_of_supply(s3_client, assessment_bucket, cos_embark_key)
        except Exception as e:
            logger.error(f"Error reading class of supply datasets: {e}")
            return {
                'statusCode': 500,
                'body': f'Error reading class of supply datasets: {e}'
            }

        logger.info(f"Distinct CLASS_OF_SUPPLY values: {cos_datasets}")

        # Update the source entity with the new ARNs
        logger.info("Updating source entity with new ARNs based on COS in Assessment")
        try:
            updated_source_entity = update_source_entity(source_entity, cos_datasets, assessment_id, aws_account_id)
        except Exception as e:
            logger.error(f"Error updating source entity: {e}")
            return {
                'statusCode': 500,
                'body': f'Error updating source entity: {e}'
            }

        logger.info(f"Updated source entity: {json.dumps(updated_source_entity, indent=4, cls=DateTimeEncoder)}")

        # Create Datasource
        try:
            physical_table_map, logical_table_map = get_table_maps(
                aws_account_id, s3_client, file_name, table_map_bucket, datasource_id
            )
            datasource_response = create_datasource(
                quicksight_client, aws_account_id, manifest_bucket, 
                object_key, datasource_id, datasource_name, owner_arn, max_retries=5
            )
            if not datasource_response:
                logger.error(f"Failed to create datasource {datasource_id}")
                logger.debug(f"Datasource creation failed response: {datasource_response}")
                return {
                    'statusCode': 500,
                    'body': f'Failed to create datasource {datasource_id}'
                }
        except Exception as e:
            logger.error(f"Error creating datasource: {e}")
            logger.debug(f"Datasource creation error details: {str(e)}")
            return {
                'statusCode': 500,
                'body': f'Error creating datasource: {e}'
            }

        # Create dataset
        try:
            dataset_response = create_dataset(
                quicksight_client, aws_account_id, datasource_id, dataset_id,
                dataset_name, physical_table_map, logical_table_map, owner_arn
            )
            if not dataset_response:
                logger.error(f"Failed to create dataset {dataset_id}")
                logger.debug(f"Dataset creation failed response: {dataset_response}")
                return {
                    'statusCode': 500,
                    'body': f'Failed to create dataset {dataset_id}'
                }
        except Exception as e:
            logger.error(f"Error creating dataset: {e}")
            logger.debug(f"Dataset creation error details: {str(e)}")
            return {
                'statusCode': 500,
                'body': f'Error creating dataset: {e}'
            }

        # Attempt to create the dashboard without checking for dataset availability
        try:
            dashboard_publish_options = {
                "AdHocFilteringOption": {
                    "AvailabilityStatus": "ENABLED"
                }
            }
            permissions = [
                {
                    "Principal": owner_arn,
                    "Actions": [
                        "quicksight:DescribeDashboard",
                        "quicksight:ListDashboardVersions",
                        "quicksight:UpdateDashboardPermissions",
                        "quicksight:QueryDashboard",
                        "quicksight:UpdateDashboard",
                        "quicksight:DeleteDashboard",
                        "quicksight:DescribeDashboardPermissions",
                        "quicksight:UpdateDashboardPublishedVersion"
                    ]
                }
            ]

            logger.info(f"Attempting to create Dashboard: {dashboard_name}.")
            dashboard_response = create_dashboard(quicksight_client, aws_account_id, dashboard_id, dashboard_name, updated_source_entity, dashboard_publish_options, permissions)

            if dashboard_response:
                logger.info(f"Dashboard created successfully: {json.dumps(dashboard_response, indent=4)}")

                # Wait for the dashboard creation to complete
                if wait_for_dashboard_creation(quicksight_client, aws_account_id, dashboard_id):
                    logger.info(f"Dashboard {dashboard_id} creation completed successfully.")
                else:
                    logger.error(f"Dashboard {dashboard_id} creation failed.")
                    return {
                        'statusCode': 500,
                        'body': f"Dashboard {dashboard_id} creation failed."
                    }
            else:
                logger.error("Failed to create dashboard.")
                return {
                    'statusCode': 500,
                    'body': 'Failed to create dashboard.'
                }

        except quicksight_client.exceptions.ResourceExistsException:
            logger.info(f"Dashboard {dashboard_id} already exists. Skipping creation.")
        except Exception as e:
            logger.error(f"Error during dashboard creation process: {str(e)}")
            return {
                'statusCode': 500,
                'body': f"Error during dashboard creation process: {str(e)}"
            }

        # Generate parameters based on file existence
        logger.info("Generating parameters based on file existence")
        try:
            version_number = get_dashboard_version(quicksight_client, aws_account_id, dashboard_id)
            parameters = update_dashboard_parameters(dashboard_sheet_map, version_number, cos_datasets)
            logger.info(f"Parameters generated for dashboard update: {parameters}")

            # Update the QuickSight dashboard with new parameters
            logger.info("Updating the QuickSight dashboard with new parameters")
            update_response = update_dashboard(
                quicksight_client, aws_account_id, dashboard_id, dashboard_name,
                parameters, dashboard_publish_options, permissions
            )
            logger.info(f"Update Dashboard Response: {update_response}")
        except Exception as e:
            logger.error(f"Error updating dashboard with new parameters: {e}")
            return {
                'statusCode': 500,
                'body': f'Error updating dashboard with new parameters: {e}'
            }

        return {
            'statusCode': 200,
            'body': 'Dashboard Processing Complete'
        }

```


